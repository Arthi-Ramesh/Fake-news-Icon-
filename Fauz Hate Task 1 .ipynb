{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79f57528-63d1-456c-a334-fcb3a3635258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760ff170-bb81-4dcb-9cbf-03db5cdba5fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Target</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER Abhi tak 2000 ke note me mujhe GPS nano c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER Abe katiye tumse kuch huaa toh jata ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER Ye sab sazish hai bina saman ke koi kaise...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abe jao tum to dasko pahle hi fash gye the jab...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ab ye afbah kaun faila Raha hai ki Shahhen bag...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Fake  Hate  Target  \\\n",
       "0  USER Abhi tak 2000 ke note me mujhe GPS nano c...     0     0       0   \n",
       "1  USER USER Abe katiye tumse kuch huaa toh jata ...     1     1       2   \n",
       "2  USER Ye sab sazish hai bina saman ke koi kaise...     1     1       2   \n",
       "3  abe jao tum to dasko pahle hi fash gye the jab...     1     0       1   \n",
       "4  Ab ye afbah kaun faila Raha hai ki Shahhen bag...     0     0       0   \n",
       "\n",
       "   Severity  \n",
       "0         0  \n",
       "1         3  \n",
       "2         2  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ARTHI\\Downloads\\cleaned_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a12f816a-71e6-4adf-a767-1f266e4cba41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(df['Text'].isnull().sum())  # Number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6d3aa88-fee2-435d-8497-98f7d8affa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values in the 'Text' column\n",
    "df['Text'] = df['Text'].fillna('')  \n",
    "X_text = df['Text']  \n",
    "y = df['Fake']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95aba307-4c20-47c3-8922-8d4aa7fcb0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd344d50-718a-47b7-8777-7a1318b0e592",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a508a9-b3ed-4f1f-914a-4fb2e3f40ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34dc9de9-816e-4cd4-895b-1bdf4348d17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d86129c3-4afa-434b-a957-526c20ba002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6953125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.25      0.38       463\n",
      "           1       0.69      0.95      0.80       817\n",
      "\n",
      "    accuracy                           0.70      1280\n",
      "   macro avg       0.71      0.60      0.59      1280\n",
      "weighted avg       0.70      0.70      0.65      1280\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[117 346]\n",
      " [ 44 773]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abeee112-1ea3-4034-8d0c-8659339dcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e943f95-1d3a-44d7-b1e2-95e7f5d828b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\ARTHI\\Downloads\\cleaned_data.csv\") \n",
    "# Step 2: Handle missing values in 'Text' column\n",
    "df['Text'] = df['Text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dd7ea18-f368-45b8-8ef5-6b6c12991bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hate</th>\n",
       "      <th>Target</th>\n",
       "      <th>Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USER Abhi tak 2000 ke note me mujhe GPS nano c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USER USER Abe katiye tumse kuch huaa toh jata ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USER Ye sab sazish hai bina saman ke koi kaise...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abe jao tum to dasko pahle hi fash gye the jab...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ab ye afbah kaun faila Raha hai ki Shahhen bag...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Fake  Hate  Target  \\\n",
       "0  USER Abhi tak 2000 ke note me mujhe GPS nano c...     0     0       0   \n",
       "1  USER USER Abe katiye tumse kuch huaa toh jata ...     1     1       2   \n",
       "2  USER Ye sab sazish hai bina saman ke koi kaise...     1     1       2   \n",
       "3  abe jao tum to dasko pahle hi fash gye the jab...     1     0       1   \n",
       "4  Ab ye afbah kaun faila Raha hai ki Shahhen bag...     0     0       0   \n",
       "\n",
       "   Severity  \n",
       "0         0  \n",
       "1         3  \n",
       "2         2  \n",
       "3         1  \n",
       "4         0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48d78f17-a0a6-44c3-bb42-06c179925846",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text = df['Text']  \n",
    "y_fake = df['Fake']  \n",
    "y_hate = df['Hate']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8093e14b-fde2-42c3-b7bb-444bdabd3137",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # Adjust max_features as needed\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "836c5185-8ce7-45d1-8ad3-40a34b3890e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_fake_train, y_fake_test = train_test_split(X_tfidf, y_fake, test_size=0.2, random_state=42)\n",
    "_, _, y_hate_train, y_hate_test = train_test_split(X_tfidf, y_hate, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d83c973-c2b7-469a-81e1-3e4839f36e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Fake classification\n",
    "nb_fake = MultinomialNB()\n",
    "nb_fake.fit(X_train, y_fake_train)\n",
    "y_fake_pred = nb_fake.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7f0140fd-a827-4794-8f56-fa2d59fe9c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Hate Classification\n",
    "nb_hate = MultinomialNB()\n",
    "nb_hate.fit(X_train, y_hate_train)\n",
    "y_hate_pred = nb_hate.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4bff3dfd-c9a2-4476-b33d-716f4d65f061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Fake Detection  \n",
      "Accuracy: 0.70234375\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.26      0.39       463\n",
      "           1       0.69      0.95      0.80       817\n",
      "\n",
      "    accuracy                           0.70      1280\n",
      "   macro avg       0.73      0.61      0.59      1280\n",
      "weighted avg       0.72      0.70      0.65      1280\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[120 343]\n",
      " [ 38 779]]\n"
     ]
    }
   ],
   "source": [
    "print(\"    Fake Detection  \")\n",
    "print(\"Accuracy:\", accuracy_score(y_fake_test, y_fake_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_fake_test, y_fake_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_fake_test, y_fake_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d86c0878-cd8a-4904-b707-3858711fd4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hate Detection \n",
      "Accuracy: 0.728125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.67      0.70       620\n",
      "           1       0.71      0.79      0.75       660\n",
      "\n",
      "    accuracy                           0.73      1280\n",
      "   macro avg       0.73      0.73      0.73      1280\n",
      "weighted avg       0.73      0.73      0.73      1280\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[413 207]\n",
      " [141 519]]\n"
     ]
    }
   ],
   "source": [
    "print(\" Hate Detection \")\n",
    "print(\"Accuracy:\", accuracy_score(y_hate_test, y_hate_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_hate_test, y_hate_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_hate_test, y_hate_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "47513cf1-93f7-43d0-b2ad-a319f5f4198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "df['Text'] = df['Text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "78b929a7-9ec2-49cf-81fd-5b39b1b4391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ed17d37f-0898-4ba0-812e-66e0e05b1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fdffb9dd-0c9c-48ac-8e1b-966984527eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, max_df=0.9, max_features=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6902297e-c372-46d1-b965-b185a8349d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB(alpha=0.5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(alpha=0.5)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB(alpha=0.5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_fake = MultinomialNB(alpha=0.5)\n",
    "nb_fake.fit(X_train, y_fake_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d024d3e3-e8d9-4e8b-9dec-c84740dba331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "selector = SelectKBest(chi2, k=500)  # Select top 500 features\n",
    "X_train_new = selector.fit_transform(X_train, y_fake_train)\n",
    "X_test_new = selector.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8d10d672-0cc9-4eee-9790-e36fabc9f467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy:  0.7188868256450351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(nb_fake, X_tfidf, y_fake, cv=5)\n",
    "print(\"Cross-Validation Accuracy: \", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "29299a57-22c2-4bfa-8156-5423815f7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d9bca0f-103e-462e-8791-4fb93ad4f090",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000, stop_words='english')  # Use unigrams and bigrams\n",
    "X_vectorized = vectorizer.fit_transform(X_text)\n",
    "\n",
    "# Step 6: Split dataset into training and testing sets\n",
    "X_train, X_test, y_fake_train, y_fake_test = train_test_split(X_vectorized, y_fake, test_size=0.2, random_state=42)\n",
    "_, _, y_hate_train, y_hate_test = train_test_split(X_vectorized, y_hate, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bfb2f9b1-d9e4-4c09-881a-d372ec47712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Fake Classification\n",
    "nb_fake = MultinomialNB()\n",
    "nb_fake.fit(X_train, y_fake_train)\n",
    "y_fake_pred = nb_fake.predict(X_test)\n",
    "\n",
    "# Model 2: Hate Classification\n",
    "nb_hate = MultinomialNB()\n",
    "nb_hate.fit(X_train, y_hate_train)\n",
    "y_hate_pred = nb_hate.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2f835f64-6b64-41e6-9b74-70faa344fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Classification:\n",
      "Accuracy: 0.72109375\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.60      0.61       463\n",
      "           1       0.78      0.79      0.78       817\n",
      "\n",
      "    accuracy                           0.72      1280\n",
      "   macro avg       0.70      0.69      0.70      1280\n",
      "weighted avg       0.72      0.72      0.72      1280\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[276 187]\n",
      " [170 647]]\n",
      "\n",
      "Hate Classification:\n",
      "Accuracy: 0.75703125\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.79      0.76       620\n",
      "           1       0.78      0.73      0.76       660\n",
      "\n",
      "    accuracy                           0.76      1280\n",
      "   macro avg       0.76      0.76      0.76      1280\n",
      "weighted avg       0.76      0.76      0.76      1280\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[487 133]\n",
      " [178 482]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Fake Classification:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_fake_test, y_fake_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_fake_test, y_fake_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_fake_test, y_fake_pred))\n",
    "\n",
    "print(\"\\nHate Classification:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_hate_test, y_hate_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_hate_test, y_hate_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_hate_test, y_hate_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51bd0115-acef-40f5-bd5c-d7d634b97443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7818403412553321\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.78      0.78       838\n",
      "           1       0.77      0.79      0.78       803\n",
      "\n",
      "    accuracy                           0.78      1641\n",
      "   macro avg       0.78      0.78      0.78      1641\n",
      "weighted avg       0.78      0.78      0.78      1641\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[651 187]\n",
      " [171 632]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Preprocessing Text\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorizing Text\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=10000, min_df=3, max_df=0.9, stop_words='english')\n",
    "X_vectorized = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Balancing Dataset (Oversampling)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_balanced, y_fake_balanced = ros.fit_resample(X_vectorized, df['Fake'])\n",
    "\n",
    "# Splitting Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_fake_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model Training\n",
    "nb = MultinomialNB(alpha=0.5)  # Adjust alpha based on GridSearchCV results\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = nb.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ad59e64-eaa5-458f-b5b9-aedaed0371fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),  \n",
    "    max_features=10000,  \n",
    "    min_df=2,            \n",
    "    max_df=0.85,        \n",
    "    stop_words='english'\n",
    ")\n",
    "X_vectorized = vectorizer.fit_transform(df['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6c8586a0-773a-4e5b-bb5c-e1f602599853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_vectorized, df['Hate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5850a33-6aab-4c34-8727-bd43ee05353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97e8ca18-f442-4215-beaf-087577869f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train the model\n",
    "nb_hate = MultinomialNB()\n",
    "nb_hate.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nb_hate.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "45e26d98-0e2d-4ed4-8c97-70c9fd589de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7749049429657795\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.78      0.77       653\n",
      "           1       0.78      0.77      0.78       662\n",
      "\n",
      "    accuracy                           0.77      1315\n",
      "   macro avg       0.77      0.77      0.77      1315\n",
      "weighted avg       0.77      0.77      0.77      1315\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[509 144]\n",
      " [152 510]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "169ee7a1-8192-4a46-93dd-ca8f3fbf0cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "validation_data = pd.read_excel(r\"C:\\Users\\ARTHI\\Downloads\\Val_Task_A.xlsx\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a4d1de0f-e31b-4861-93d6-084cf0d56428",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data['Tweet'] = validation_data['Tweet'].fillna('')  # Handle missing values\n",
    "validation_data['Tweet'] = validation_data['Tweet'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f9a8935a-6b30-4764-a481-0af34a7fe8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_vectorized = vectorizer.transform(validation_data['Tweet'])\n",
    "y_fake_pred = nb.predict(X_validation_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d63b16e7-96b5-4921-a0f4-9aea17ef36f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined predictions saved to 'C:\\Users\\ARTHI\\Downloads\\validation_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions for fake and hate classifications\n",
    "y_fake_pred = nb_fake.predict(X_validation_vectorized)\n",
    "y_hate_pred = nb_hate.predict(X_validation_vectorized)\n",
    "\n",
    "# Add predictions to the validation dataset\n",
    "validation_data['Fake_Prediction'] = y_fake_pred\n",
    "validation_data['Hate_Prediction'] = y_hate_pred\n",
    "\n",
    "# Save the combined predictions to a single CSV file\n",
    "output_path = r\"C:\\Users\\ARTHI\\Downloads\\validation_predictions.csv\"\n",
    "validation_data[['Tweet', 'Fake_Prediction', 'Hate_Prediction']].to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined predictions saved to '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a84cb1f0-81f0-477e-8a2b-3f9bd7f1de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Fake and Hate predictions saved to 'C:\\Users\\ARTHI\\Downloads\\test_fake_hate_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_file_path = r\"C:\\Users\\ARTHI\\Downloads\\Test_Task_A.xlsx\"\n",
    "test_data = pd.read_excel(test_file_path)\n",
    "\n",
    "test_data['Tweet'] = test_data['Tweet'].fillna(\"\")\n",
    "\n",
    "X_test_vectorized = vectorizer.transform(test_data['Tweet'])\n",
    "\n",
    "\n",
    "y_fake_pred_test = nb_fake.predict(X_test_vectorized)  # Replace with your trained Fake model\n",
    "y_hate_pred_test = nb_hate.predict(X_test_vectorized)  # Replace with your trained Hate model\n",
    "\n",
    "test_data['Fake_Prediction'] = y_fake_pred_test\n",
    "test_data['Hate_Prediction'] = y_hate_pred_test\n",
    "\n",
    "\n",
    "output_path_test = r\"C:\\Users\\ARTHI\\Downloads\\test_fake_hate_predictions.csv\"\n",
    "test_data[['Tweet', 'Fake_Prediction', 'Hate_Prediction']].to_csv(output_path_test, index=False)\n",
    "\n",
    "print(f\"Test Fake and Hate predictions saved to '{output_path_test}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4685ae55-ef60-494d-be97-7578cde8a49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
